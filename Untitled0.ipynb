{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielstengel/emotional-dataset/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "0eonZOVYFTUE",
        "colab_type": "code",
        "outputId": "2e40c6c9-8f2a-48bf-be30-1c44c4b41737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# import the most useful packages\n",
        "import numpy as np \n",
        "import matplotlib\n",
        "import math\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import IPython.display as ipd\n",
        "print('finished importing')\n",
        "! pip install librosa\n",
        "import librosa\n",
        "import librosa.display"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finished importing\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.20.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.13.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.3.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.11.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.27.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "myVyLPMkFlf_",
        "colab_type": "code",
        "outputId": "55435a0e-f464-4b00-fdaa-66431568dcf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wAM-w4C6FmJR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make a folder for this course and this project. \n",
        "# the -p flag says make any needed parent folders and  don't complain about existing ones.\n",
        "my_dir = '/content/drive/My Drive/IW06/EmotionProsody'\n",
        "! mkdir -p '$my_dir'\n",
        "# this command is equivalent to \"cd\" that sets a working directory\n",
        "os.chdir(my_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLfeKPlr4zvK",
        "colab_type": "code",
        "outputId": "9d65e705-726e-4294-8bbf-f0bd9d17bab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "# first step, download Audio MNIST dataset\n",
        "! git clone https://github.com/gabrielstengel/emotional-dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'emotional-dataset'...\n",
            "remote: Enumerating objects: 4792, done.\u001b[K\n",
            "remote: Counting objects: 100% (4792/4792), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4785/4785), done.\u001b[K\n",
            "remote: Total 4792 (delta 9), reused 4788 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (4792/4792), 694.22 MiB | 24.69 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "Checking out files: 100% (15390/15390), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xU5dnUtL7wDf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now detect audio files for each type of classification:\n",
        "\n",
        "\n",
        "1.   **two-labeled**: contains valenced versus unvalenced speech. \n",
        "2.   **three-labelled:** contains positive, neutral, and negative speech.\n",
        "3.   **four-basic**: contains the four basic emotions most searched for -- anger, happyness, neutral, and sadness.\n",
        "4.   **all-labelled**: contains every distinct emotional valence"
      ]
    },
    {
      "metadata": {
        "id": "cRqaZ3wv45q8",
        "colab_type": "code",
        "outputId": "4e7a1372-0ae5-451d-fede-647a6f73ab7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# detect audio files\n",
        "from os import listdir\n",
        "from os.path import join, isfile\n",
        "def scan_files(mypath, ext):\n",
        "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f)) and f.endswith(ext)]\n",
        "    return onlyfiles\n",
        "files_two = scan_files('emotional-dataset/two-labeled', '.wav')\n",
        "files_three = scan_files('emotional-dataset/three-labeled', '.wav')\n",
        "files_four = scan_files('emotional-dataset/four-labeled', '.wav')\n",
        "files_all = scan_files('emotional-dataset/all-labeled', '.wav')\n",
        "\n",
        "print(len(files_two), len(files_three), len(files_four), len(files_all))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4773 3528 2314 4773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3xgLMRNa4-Fm",
        "colab_type": "code",
        "outputId": "4536ae78-ebd7-4fb6-dde5-63af4efc5fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2669
        }
      },
      "cell_type": "code",
      "source": [
        "# load into data matrix\n",
        "\n",
        "SR = 8000\n",
        "MAX_AUDIO_LEN = SR * 2\n",
        "MAX_MFCC_LEN = 20\n",
        "\n",
        "\n",
        "audiodata_two = []\n",
        "mfccdata_two = []\n",
        "labels_two = []\n",
        "\n",
        "\n",
        "for i, f in enumerate(files_two):\n",
        "   \n",
        "  \n",
        "  file_name = join('emotional-dataset/two-labeled', f)\n",
        "  audio, sample_rate = librosa.core.load(file_name, sr=SR)\n",
        "  \n",
        "  audio = audio[:MAX_AUDIO_LEN]\n",
        "  pad_width = MAX_AUDIO_LEN - len(audio)\n",
        "  audio = np.pad(audio, pad_width=(0, pad_width), mode='constant')\n",
        "  audiodata_two.append(audio)\n",
        "  \n",
        "  mfcc = librosa.feature.mfcc(audio, sr=SR).T\n",
        "  mfccdata_two.append(mfcc)\n",
        "  \n",
        "  labels_two.append(int(f.split('_')[0]))\n",
        "  if i % 100 ==0:\n",
        "    print(i, len(audio), file_name, int(f.split('_')[0]))\n",
        "\n",
        "audiodata_two = np.asarray(audiodata_two)\n",
        "mfccdata_two = np.asarray(mfccdata_two)\n",
        "labels_two = np.asarray(labels_two)\n",
        "\n",
        "\n",
        "audiodata_three = []\n",
        "mfccdata_three = []\n",
        "labels_three = []\n",
        "\n",
        "\n",
        "for i, f in enumerate(files_three):\n",
        "   \n",
        "  \n",
        "  file_name = join('emotional-dataset/three-labeled', f)\n",
        "  audio, sample_rate = librosa.core.load(file_name, sr=SR)\n",
        "  \n",
        "  audio = audio[:MAX_AUDIO_LEN]\n",
        "  pad_width = MAX_AUDIO_LEN - len(audio)\n",
        "  audio = np.pad(audio, pad_width=(0, pad_width), mode='constant')\n",
        "  audiodata_three.append(audio)\n",
        "  \n",
        "  mfcc = librosa.feature.mfcc(audio, sr=SR).T\n",
        "  mfccdata_three.append(mfcc)\n",
        "  \n",
        "  labels_three.append(int(f.split('_')[0]))\n",
        "  if i % 100 ==0:\n",
        "    print(i, len(audio), file_name, int(f.split('_')[0]))\n",
        "\n",
        "audiodata_three = np.asarray(audiodata_three)\n",
        "mfccdata_three = np.asarray(mfccdata_three)\n",
        "labels_three = np.asarray(labels_three)\n",
        "\n",
        "\n",
        "\n",
        "audiodata_four = []\n",
        "mfccdata_four = []\n",
        "labels_four = []\n",
        "\n",
        "\n",
        "for i, f in enumerate(files_four):\n",
        "   \n",
        "  \n",
        "  file_name = join('emotional-dataset/four-labeled', f)\n",
        "  audio, sample_rate = librosa.core.load(file_name, sr=SR)\n",
        "  \n",
        "  audio = audio[:MAX_AUDIO_LEN]\n",
        "  pad_width = MAX_AUDIO_LEN - len(audio)\n",
        "  audio = np.pad(audio, pad_width=(0, pad_width), mode='constant')\n",
        "  audiodata_four.append(audio)\n",
        "  \n",
        "  mfcc = librosa.feature.mfcc(audio, sr=SR).T\n",
        "  mfccdata_four.append(mfcc)\n",
        "  \n",
        "  labels_four.append(int(f.split('_')[0]))\n",
        "  if i % 100 ==0:\n",
        "    print(i, len(audio), file_name, int(f.split('_')[0]))\n",
        "\n",
        "audiodata_four = np.asarray(audiodata_four)\n",
        "mfccdata_four = np.asarray(mfccdata_four)\n",
        "labels_four = np.asarray(labels_four)\n",
        "\n",
        "\n",
        "\n",
        "audiodata_all = []\n",
        "mfccdata_all = []\n",
        "labels_all = []\n",
        "\n",
        "for i, f in enumerate(files_all):\n",
        "   \n",
        "  \n",
        "  file_name = join('emotional-dataset/all-labeled', f)\n",
        "  audio, sample_rate = librosa.core.load(file_name, sr=SR)\n",
        "  \n",
        "  audio = audio[:MAX_AUDIO_LEN]\n",
        "  pad_width = MAX_AUDIO_LEN - len(audio)\n",
        "  audio = np.pad(audio, pad_width=(0, pad_width), mode='constant')\n",
        "  audiodata_all.append(audio)\n",
        "  \n",
        "  mfcc = librosa.feature.mfcc(audio, sr=SR).T\n",
        "  mfccdata_all.append(mfcc)\n",
        "  \n",
        "  labels_all.append(int(f.split('_')[0]))\n",
        "  if i % 100 ==0:\n",
        "    print(i, len(audio), file_name, int(f.split('_')[0]))\n",
        "\n",
        "audiodata_all = np.asarray(audiodata_all)\n",
        "mfccdata_all = np.asarray(mfccdata_all)\n",
        "labels_all = np.asarray(labels_all)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 16000 emotional-dataset/two-labeled/1_PRIDE_1655.wav 1\n",
            "100 16000 emotional-dataset/two-labeled/1_BOREDOM_205.wav 1\n",
            "200 16000 emotional-dataset/two-labeled/1_PASSIVE_1868.wav 1\n",
            "300 16000 emotional-dataset/two-labeled/1_CONTEMPT_273.wav 1\n",
            "400 16000 emotional-dataset/two-labeled/1_ANXIETY_2657.wav 1\n",
            "500 16000 emotional-dataset/two-labeled/1_CONTEMPT_4424.wav 1\n",
            "600 16000 emotional-dataset/two-labeled/0_NEUTRAL_861.wav 0\n",
            "700 16000 emotional-dataset/two-labeled/1_DOMINANT_331.wav 1\n",
            "800 16000 emotional-dataset/two-labeled/1_PASSIVE_2387.wav 1\n",
            "900 16000 emotional-dataset/two-labeled/1_PASSIVE_4617.wav 1\n",
            "1000 16000 emotional-dataset/two-labeled/0_NEUTRAL_4691.wav 0\n",
            "1100 16000 emotional-dataset/two-labeled/0_NEUTRAL_4688.wav 0\n",
            "1200 16000 emotional-dataset/two-labeled/0_NEUTRAL_899.wav 0\n",
            "1300 16000 emotional-dataset/two-labeled/1_ELATION_2193.wav 1\n",
            "1400 16000 emotional-dataset/two-labeled/1_PANIC_522.wav 1\n",
            "1500 16000 emotional-dataset/two-labeled/0_NEUTRAL_455.wav 0\n",
            "1600 16000 emotional-dataset/two-labeled/1_CONTEMPT_827.wav 1\n",
            "1700 16000 emotional-dataset/two-labeled/1_CONTEMPT_2913.wav 1\n",
            "1800 16000 emotional-dataset/two-labeled/0_NEUTRAL_3419.wav 0\n",
            "1900 16000 emotional-dataset/two-labeled/0_NEUTRAL_3884.wav 0\n",
            "2000 16000 emotional-dataset/two-labeled/1_DOMINANT_3040.wav 1\n",
            "2100 16000 emotional-dataset/two-labeled/1_HAPPY_1553.wav 1\n",
            "2200 16000 emotional-dataset/two-labeled/1_ANXIETY_2667.wav 1\n",
            "2300 16000 emotional-dataset/two-labeled/0_NEUTRAL_1252.wav 0\n",
            "2400 16000 emotional-dataset/two-labeled/0_NEUTRAL_4091.wav 0\n",
            "2500 16000 emotional-dataset/two-labeled/0_NEUTRAL_1109.wav 0\n",
            "2600 16000 emotional-dataset/two-labeled/1_BOREDOM_2255.wav 1\n",
            "2700 16000 emotional-dataset/two-labeled/1_DISGUST_47.wav 1\n",
            "2800 16000 emotional-dataset/two-labeled/1_PANIC_2080.wav 1\n",
            "2900 16000 emotional-dataset/two-labeled/1_HOT-ANGER_568.wav 1\n",
            "3000 16000 emotional-dataset/two-labeled/1_PANIC_1350.wav 1\n",
            "3100 16000 emotional-dataset/two-labeled/0_NEUTRAL_1938.wav 0\n",
            "3200 16000 emotional-dataset/two-labeled/0_NEUTRAL_4151.wav 0\n",
            "3300 16000 emotional-dataset/two-labeled/0_NEUTRAL_1189.wav 0\n",
            "3400 16000 emotional-dataset/two-labeled/0_NEUTRAL_2462.wav 0\n",
            "3500 16000 emotional-dataset/two-labeled/1_PASSIVE_1089.wav 1\n",
            "3600 16000 emotional-dataset/two-labeled/1_BOREDOM_4373.wav 1\n",
            "3700 16000 emotional-dataset/two-labeled/0_NEUTRAL_4498.wav 0\n",
            "3800 16000 emotional-dataset/two-labeled/1_DOMINANT_1823.wav 1\n",
            "3900 16000 emotional-dataset/two-labeled/1_COLD-ANGER_4250.wav 1\n",
            "4000 16000 emotional-dataset/two-labeled/0_NEUTRAL_3391.wav 0\n",
            "4100 16000 emotional-dataset/two-labeled/1_COLD-ANGER_2712.wav 1\n",
            "4200 16000 emotional-dataset/two-labeled/0_NEUTRAL_297.wav 0\n",
            "4300 16000 emotional-dataset/two-labeled/1_SHAME_231.wav 1\n",
            "4400 16000 emotional-dataset/two-labeled/1_HAPPY_3740.wav 1\n",
            "4500 16000 emotional-dataset/two-labeled/0_NEUTRAL_3400.wav 0\n",
            "4600 16000 emotional-dataset/two-labeled/0_NEUTRAL_3273.wav 0\n",
            "4700 16000 emotional-dataset/two-labeled/1_ELATION_3627.wav 1\n",
            "0 16000 emotional-dataset/three-labeled/2_SADNESS_143.wav 2\n",
            "100 16000 emotional-dataset/three-labeled/1_NEUTRAL_1283.wav 1\n",
            "200 16000 emotional-dataset/three-labeled/1_NEUTRAL_1995.wav 1\n",
            "300 16000 emotional-dataset/three-labeled/2_CONTEMPT_2919.wav 2\n",
            "400 16000 emotional-dataset/three-labeled/2_DISGUST_502.wav 2\n",
            "500 16000 emotional-dataset/three-labeled/2_CONTEMPT_271.wav 2\n",
            "600 16000 emotional-dataset/three-labeled/1_NEUTRAL_4042.wav 1\n",
            "700 16000 emotional-dataset/three-labeled/1_NEUTRAL_1259.wav 1\n",
            "800 16000 emotional-dataset/three-labeled/2_ANXIETY_558.wav 2\n",
            "900 16000 emotional-dataset/three-labeled/2_HOT-ANGER_92.wav 2\n",
            "1000 16000 emotional-dataset/three-labeled/1_NEUTRAL_1159.wav 1\n",
            "1100 16000 emotional-dataset/three-labeled/2_DISGUST_1294.wav 2\n",
            "1200 16000 emotional-dataset/three-labeled/1_NEUTRAL_2489.wav 1\n",
            "1300 16000 emotional-dataset/three-labeled/2_CONTEMPT_837.wav 2\n",
            "1400 16000 emotional-dataset/three-labeled/1_NEUTRAL_1751.wav 1\n",
            "1500 16000 emotional-dataset/three-labeled/2_HOT-ANGER_574.wav 2\n",
            "1600 16000 emotional-dataset/three-labeled/2_DESPAIR_626.wav 2\n",
            "1700 16000 emotional-dataset/three-labeled/3_HAPPY_3659.wav 3\n",
            "1800 16000 emotional-dataset/three-labeled/2_SHAME_778.wav 2\n",
            "1900 16000 emotional-dataset/three-labeled/2_SADNESS_2737.wav 2\n",
            "2000 16000 emotional-dataset/three-labeled/1_NEUTRAL_4479.wav 1\n",
            "2100 16000 emotional-dataset/three-labeled/1_NEUTRAL_462.wav 1\n",
            "2200 16000 emotional-dataset/three-labeled/2_PANIC_4474.wav 2\n",
            "2300 16000 emotional-dataset/three-labeled/2_CONTEMPT_2927.wav 2\n",
            "2400 16000 emotional-dataset/three-labeled/1_NEUTRAL_1953.wav 1\n",
            "2500 16000 emotional-dataset/three-labeled/1_NEUTRAL_4483.wav 1\n",
            "2600 16000 emotional-dataset/three-labeled/3_ELATION_1528.wav 3\n",
            "2700 16000 emotional-dataset/three-labeled/2_PANIC_2641.wav 2\n",
            "2800 16000 emotional-dataset/three-labeled/2_HOT-ANGER_4229.wav 2\n",
            "2900 16000 emotional-dataset/three-labeled/3_INTEREST_723.wav 3\n",
            "3000 16000 emotional-dataset/three-labeled/1_NEUTRAL_3872.wav 1\n",
            "3100 16000 emotional-dataset/three-labeled/1_NEUTRAL_1919.wav 1\n",
            "3200 16000 emotional-dataset/three-labeled/3_ELATION_1514.wav 3\n",
            "3300 16000 emotional-dataset/three-labeled/1_NEUTRAL_4732.wav 1\n",
            "3400 16000 emotional-dataset/three-labeled/2_COLD-ANGER_2711.wav 2\n",
            "3500 16000 emotional-dataset/three-labeled/3_HAPPY_1551.wav 3\n",
            "0 16000 emotional-dataset/four-labeled/3_SADNESS_3582.wav 3\n",
            "100 16000 emotional-dataset/four-labeled/4_ELATION_3628.wav 4\n",
            "200 16000 emotional-dataset/four-labeled/1_NEUTRAL_2044.wav 1\n",
            "300 16000 emotional-dataset/four-labeled/1_NEUTRAL_4731.wav 1\n",
            "400 16000 emotional-dataset/four-labeled/1_NEUTRAL_3887.wav 1\n",
            "500 16000 emotional-dataset/four-labeled/2_COLD-ANGER_611.wav 2\n",
            "600 16000 emotional-dataset/four-labeled/1_NEUTRAL_1914.wav 1\n",
            "700 16000 emotional-dataset/four-labeled/2_COLD-ANGER_586.wav 2\n",
            "800 16000 emotional-dataset/four-labeled/3_SADNESS_657.wav 3\n",
            "900 16000 emotional-dataset/four-labeled/1_NEUTRAL_4097.wav 1\n",
            "1000 16000 emotional-dataset/four-labeled/4_ELATION_2761.wav 4\n",
            "1100 16000 emotional-dataset/four-labeled/2_HOT-ANGER_2109.wav 2\n",
            "1200 16000 emotional-dataset/four-labeled/1_NEUTRAL_1761.wav 1\n",
            "1300 16000 emotional-dataset/four-labeled/1_NEUTRAL_3298.wav 1\n",
            "1400 16000 emotional-dataset/four-labeled/2_HOT-ANGER_3513.wav 2\n",
            "1500 16000 emotional-dataset/four-labeled/1_NEUTRAL_2623.wav 1\n",
            "1600 16000 emotional-dataset/four-labeled/1_NEUTRAL_3265.wav 1\n",
            "1700 16000 emotional-dataset/four-labeled/2_HOT-ANGER_91.wav 2\n",
            "1800 16000 emotional-dataset/four-labeled/1_NEUTRAL_2971.wav 1\n",
            "1900 16000 emotional-dataset/four-labeled/1_NEUTRAL_871.wav 1\n",
            "2000 16000 emotional-dataset/four-labeled/1_NEUTRAL_4730.wav 1\n",
            "2100 16000 emotional-dataset/four-labeled/4_ELATION_680.wav 4\n",
            "2200 16000 emotional-dataset/four-labeled/1_NEUTRAL_1195.wav 1\n",
            "2300 16000 emotional-dataset/four-labeled/4_HAPPY_3756.wav 4\n",
            "0 16000 emotional-dataset/all-labeled/9_HAPPY_3667.wav 9\n",
            "100 16000 emotional-dataset/all-labeled/1_DISGUST_25.wav 1\n",
            "200 16000 emotional-dataset/all-labeled/7_SADNESS_4282.wav 7\n",
            "300 16000 emotional-dataset/all-labeled/16_DOMINANT_1769.wav 16\n",
            "400 16000 emotional-dataset/all-labeled/12_SHAME_2890.wav 12\n",
            "500 16000 emotional-dataset/all-labeled/8_ELATION_1513.wav 8\n",
            "600 16000 emotional-dataset/all-labeled/15_PASSIVE_2392.wav 15\n",
            "700 16000 emotional-dataset/all-labeled/13_PRIDE_1658.wav 13\n",
            "800 16000 emotional-dataset/all-labeled/15_PASSIVE_2402.wav 15\n",
            "900 16000 emotional-dataset/all-labeled/0_NEUTRAL_3210.wav 0\n",
            "1000 16000 emotional-dataset/all-labeled/0_NEUTRAL_1254.wav 0\n",
            "1100 16000 emotional-dataset/all-labeled/0_NEUTRAL_2609.wav 0\n",
            "1200 16000 emotional-dataset/all-labeled/7_SADNESS_3592.wav 7\n",
            "1300 16000 emotional-dataset/all-labeled/0_NEUTRAL_1165.wav 0\n",
            "1400 16000 emotional-dataset/all-labeled/15_PASSIVE_1860.wav 15\n",
            "1500 16000 emotional-dataset/all-labeled/0_NEUTRAL_911.wav 0\n",
            "1600 16000 emotional-dataset/all-labeled/9_HAPPY_715.wav 9\n",
            "1700 16000 emotional-dataset/all-labeled/9_HAPPY_2792.wav 9\n",
            "1800 16000 emotional-dataset/all-labeled/13_PRIDE_2286.wav 13\n",
            "1900 16000 emotional-dataset/all-labeled/16_DOMINANT_999.wav 16\n",
            "2000 16000 emotional-dataset/all-labeled/3_ANXIETY_4208.wav 3\n",
            "2100 16000 emotional-dataset/all-labeled/15_PASSIVE_1073.wav 15\n",
            "2200 16000 emotional-dataset/all-labeled/0_NEUTRAL_4705.wav 0\n",
            "2300 16000 emotional-dataset/all-labeled/0_NEUTRAL_4102.wav 0\n",
            "2400 16000 emotional-dataset/all-labeled/16_DOMINANT_3899.wav 16\n",
            "2500 16000 emotional-dataset/all-labeled/6_DESPAIR_622.wav 6\n",
            "2600 16000 emotional-dataset/all-labeled/2_PANIC_1337.wav 2\n",
            "2700 16000 emotional-dataset/all-labeled/0_NEUTRAL_295.wav 0\n",
            "2800 16000 emotional-dataset/all-labeled/0_NEUTRAL_3271.wav 0\n",
            "2900 16000 emotional-dataset/all-labeled/15_PASSIVE_2394.wav 15\n",
            "3000 16000 emotional-dataset/all-labeled/13_PRIDE_3804.wav 13\n",
            "3100 16000 emotional-dataset/all-labeled/0_NEUTRAL_4696.wav 0\n",
            "3200 16000 emotional-dataset/all-labeled/0_NEUTRAL_4722.wav 0\n",
            "3300 16000 emotional-dataset/all-labeled/12_SHAME_4389.wav 12\n",
            "3400 16000 emotional-dataset/all-labeled/5_COLD-ANGER_3547.wav 5\n",
            "3500 16000 emotional-dataset/all-labeled/13_PRIDE_250.wav 13\n",
            "3600 16000 emotional-dataset/all-labeled/11_BOREDOM_1620.wav 11\n",
            "3700 16000 emotional-dataset/all-labeled/15_PASSIVE_3354.wav 15\n",
            "3800 16000 emotional-dataset/all-labeled/13_PRIDE_246.wav 13\n",
            "3900 16000 emotional-dataset/all-labeled/0_NEUTRAL_4060.wav 0\n",
            "4000 16000 emotional-dataset/all-labeled/1_DISGUST_3455.wav 1\n",
            "4100 16000 emotional-dataset/all-labeled/0_NEUTRAL_1902.wav 0\n",
            "4200 16000 emotional-dataset/all-labeled/0_NEUTRAL_485.wav 0\n",
            "4300 16000 emotional-dataset/all-labeled/15_PASSIVE_4008.wav 15\n",
            "4400 16000 emotional-dataset/all-labeled/16_DOMINANT_2435.wav 16\n",
            "4500 16000 emotional-dataset/all-labeled/15_PASSIVE_1859.wav 15\n",
            "4600 16000 emotional-dataset/all-labeled/0_NEUTRAL_2973.wav 0\n",
            "4700 16000 emotional-dataset/all-labeled/0_NEUTRAL_4106.wav 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1OI-9mbvx0dW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Print out the shapes of the different dataset's data:\n"
      ]
    },
    {
      "metadata": {
        "id": "tdDRChTOx1SL",
        "colab_type": "code",
        "outputId": "9ac5d0b2-4841-4837-ec9b-bca87742d7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "print('two labeled:')\n",
        "print(audiodata_two.shape, mfccdata_two.shape, labels_two.shape)\n",
        "print('three labeled:')\n",
        "print(audiodata_three.shape, mfccdata_three.shape, labels_three.shape)\n",
        "print('four labeled:')\n",
        "print(audiodata_four.shape, mfccdata_four.shape, labels_four.shape)\n",
        "print('all labeled:')\n",
        "print(audiodata_all.shape, mfccdata_all.shape, labels_all.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "two labeled:\n",
            "(4773, 16000) (4773, 32, 20) (4773,)\n",
            "three labeled:\n",
            "(3528, 16000) (3528, 32, 20) (3528,)\n",
            "four labeled:\n",
            "(2314, 16000) (2314, 32, 20) (2314,)\n",
            "all labeled:\n",
            "(4773, 16000) (4773, 32, 20) (4773,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FeNS_CtHLUVU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Split up the data into training and testing"
      ]
    },
    {
      "metadata": {
        "id": "tgkC2vYzx-3p",
        "colab_type": "code",
        "outputId": "62dd7c0f-df18-404b-bb96-5fd5ef667ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "audio_train_two, audio_test_two, mfcc_train_two, mfcc_test_two, y_train_two, y_test_two = train_test_split(audiodata_two, mfccdata_two, labels_two, test_size=0.1, random_state=1)\n",
        "audio_train_three, audio_test_three, mfcc_train_three, mfcc_test_three, y_train_three, y_test_three = train_test_split(audiodata_three, mfccdata_three, labels_three, test_size=0.1, random_state=1)\n",
        "audio_train_four, audio_test_four, mfcc_train_four, mfcc_test_four, y_train_four, y_test_four = train_test_split(audiodata_four, mfccdata_four, labels_four, test_size=0.1, random_state=1)\n",
        "audio_train_all, audio_test_all, mfcc_train_all, mfcc_test_all, y_train_all, y_test_all = train_test_split(audiodata_all, mfccdata_all, labels_all, test_size=0.1, random_state=1)\n",
        "\n",
        "print('two labeled:')\n",
        "print(audio_train_two.shape, audio_test_two.shape, mfcc_train_two.shape, mfcc_test_two.shape, y_train_two.shape, y_test_two.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "two labeled:\n",
            "(4295, 16000) (478, 16000) (4295, 32, 20) (478, 32, 20) (4295,) (478,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HnGa7BZkyB9s",
        "colab_type": "code",
        "outputId": "26e1a6c4-c581-4b20-e4c7-caded52d8ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found; please go to Edit->Notebook Settings and select GPU')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cwZS-eIHyLcw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(0.25),\n",
        "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(0.4),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ehKPyIavyOos",
        "colab_type": "code",
        "outputId": "1bba4c55-6492-44e8-fb38-e7644c2cd713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2346
        }
      },
      "cell_type": "code",
      "source": [
        "print('\\n two labeled: (random would be .5)')\n",
        "model.fit(audio_train_two, y_train_two, epochs=15, validation_split=0.1)\n",
        "model.evaluate(audio_test_two, y_test_two)\n",
        "\n",
        "print('\\n three labeled: (random would be .33)')\n",
        "model.fit(audio_train_three, y_train_three, epochs=15, validation_split=0.1)\n",
        "model.evaluate(audio_test_three, y_test_three)\n",
        "\n",
        "print('\\n four labeled: (random would be .25)')\n",
        "model.fit(audio_train_four, y_train_four, epochs=15, validation_split=0.1)\n",
        "model.evaluate(audio_test_four, y_test_four)\n",
        "\n",
        "print('\\n all labeled: (random would be 0.058)')\n",
        "model.fit(audio_train_all, y_train_all, epochs=15, validation_split=0.1)\n",
        "model.evaluate(audio_test_all, y_test_all)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " two labeled: (random would be .5)\n",
            "Train on 3865 samples, validate on 430 samples\n",
            "Epoch 1/15\n",
            "3865/3865 [==============================] - 2s 579us/step - loss: 0.9801 - acc: 0.6776 - val_loss: 0.6373 - val_acc: 0.6837\n",
            "Epoch 2/15\n",
            "3865/3865 [==============================] - 1s 381us/step - loss: 0.4259 - acc: 0.7920 - val_loss: 0.8420 - val_acc: 0.6116\n",
            "Epoch 3/15\n",
            "3865/3865 [==============================] - 1s 381us/step - loss: 0.2146 - acc: 0.9169 - val_loss: 1.2233 - val_acc: 0.6186\n",
            "Epoch 4/15\n",
            "3865/3865 [==============================] - 1s 382us/step - loss: 0.0848 - acc: 0.9728 - val_loss: 1.7254 - val_acc: 0.6186\n",
            "Epoch 5/15\n",
            "3865/3865 [==============================] - 1s 378us/step - loss: 0.0304 - acc: 0.9930 - val_loss: 2.1106 - val_acc: 0.6186\n",
            "Epoch 6/15\n",
            "3865/3865 [==============================] - 1s 377us/step - loss: 0.0449 - acc: 0.9889 - val_loss: 2.2764 - val_acc: 0.6186\n",
            "Epoch 7/15\n",
            "3865/3865 [==============================] - 1s 383us/step - loss: 0.0758 - acc: 0.9785 - val_loss: 2.2514 - val_acc: 0.6186\n",
            "Epoch 8/15\n",
            "3865/3865 [==============================] - 1s 382us/step - loss: 0.0380 - acc: 0.9928 - val_loss: 2.5074 - val_acc: 0.6116\n",
            "Epoch 9/15\n",
            "3865/3865 [==============================] - 1s 384us/step - loss: 0.0249 - acc: 0.9959 - val_loss: 2.5649 - val_acc: 0.6140\n",
            "Epoch 10/15\n",
            "3865/3865 [==============================] - 1s 381us/step - loss: 0.0154 - acc: 0.9969 - val_loss: 2.7287 - val_acc: 0.6349\n",
            "Epoch 11/15\n",
            "3865/3865 [==============================] - 1s 380us/step - loss: 0.0308 - acc: 0.9940 - val_loss: 2.8257 - val_acc: 0.5930\n",
            "Epoch 12/15\n",
            "3865/3865 [==============================] - 1s 378us/step - loss: 0.0476 - acc: 0.9920 - val_loss: 2.6855 - val_acc: 0.6116\n",
            "Epoch 13/15\n",
            "3865/3865 [==============================] - 1s 380us/step - loss: 0.0187 - acc: 0.9974 - val_loss: 2.9464 - val_acc: 0.5930\n",
            "Epoch 14/15\n",
            "3865/3865 [==============================] - 1s 377us/step - loss: 0.0087 - acc: 0.9979 - val_loss: 3.0384 - val_acc: 0.5907\n",
            "Epoch 15/15\n",
            "3865/3865 [==============================] - 1s 378us/step - loss: 0.0056 - acc: 0.9997 - val_loss: 3.1353 - val_acc: 0.5977\n",
            "478/478 [==============================] - 0s 127us/step\n",
            "\n",
            " three labeled: (random would be .33)\n",
            "Train on 2857 samples, validate on 318 samples\n",
            "Epoch 1/15\n",
            "2857/2857 [==============================] - 1s 374us/step - loss: 2.0915 - acc: 0.6150 - val_loss: 0.6606 - val_acc: 0.7516\n",
            "Epoch 2/15\n",
            "2857/2857 [==============================] - 1s 379us/step - loss: 0.5758 - acc: 0.8145 - val_loss: 0.7215 - val_acc: 0.7233\n",
            "Epoch 3/15\n",
            "2857/2857 [==============================] - 1s 378us/step - loss: 0.2728 - acc: 0.9230 - val_loss: 0.8312 - val_acc: 0.7233\n",
            "Epoch 4/15\n",
            "2857/2857 [==============================] - 1s 380us/step - loss: 0.1303 - acc: 0.9650 - val_loss: 1.0273 - val_acc: 0.6918\n",
            "Epoch 5/15\n",
            "2857/2857 [==============================] - 1s 381us/step - loss: 0.0638 - acc: 0.9898 - val_loss: 1.2131 - val_acc: 0.6950\n",
            "Epoch 6/15\n",
            "2857/2857 [==============================] - 1s 381us/step - loss: 0.0365 - acc: 0.9968 - val_loss: 1.3817 - val_acc: 0.6824\n",
            "Epoch 7/15\n",
            "2857/2857 [==============================] - 1s 377us/step - loss: 0.0249 - acc: 0.9979 - val_loss: 1.5093 - val_acc: 0.6698\n",
            "Epoch 8/15\n",
            "2857/2857 [==============================] - 1s 376us/step - loss: 0.0194 - acc: 0.9993 - val_loss: 1.6013 - val_acc: 0.6635\n",
            "Epoch 9/15\n",
            "2857/2857 [==============================] - 1s 381us/step - loss: 0.0167 - acc: 0.9993 - val_loss: 1.6816 - val_acc: 0.6572\n",
            "Epoch 10/15\n",
            "2857/2857 [==============================] - 1s 382us/step - loss: 0.0151 - acc: 0.9993 - val_loss: 1.7499 - val_acc: 0.6572\n",
            "Epoch 11/15\n",
            "2857/2857 [==============================] - 1s 383us/step - loss: 0.0141 - acc: 0.9993 - val_loss: 1.8119 - val_acc: 0.6541\n",
            "Epoch 12/15\n",
            "2857/2857 [==============================] - 1s 380us/step - loss: 0.0135 - acc: 0.9993 - val_loss: 1.8615 - val_acc: 0.6541\n",
            "Epoch 13/15\n",
            "2857/2857 [==============================] - 1s 379us/step - loss: 0.0130 - acc: 0.9993 - val_loss: 1.9090 - val_acc: 0.6541\n",
            "Epoch 14/15\n",
            "2857/2857 [==============================] - 1s 376us/step - loss: 0.0127 - acc: 0.9993 - val_loss: 1.9539 - val_acc: 0.6509\n",
            "Epoch 15/15\n",
            "2857/2857 [==============================] - 1s 378us/step - loss: 0.0124 - acc: 0.9993 - val_loss: 1.9941 - val_acc: 0.6509\n",
            "353/353 [==============================] - 0s 123us/step\n",
            "\n",
            " four labeled: (random would be .25)\n",
            "Train on 1873 samples, validate on 209 samples\n",
            "Epoch 1/15\n",
            "1873/1873 [==============================] - 1s 381us/step - loss: 1.5023 - acc: 0.7993 - val_loss: 0.4859 - val_acc: 0.8708\n",
            "Epoch 2/15\n",
            "1873/1873 [==============================] - 1s 379us/step - loss: 0.3554 - acc: 0.9156 - val_loss: 0.4732 - val_acc: 0.7895\n",
            "Epoch 3/15\n",
            "1873/1873 [==============================] - 1s 376us/step - loss: 0.1798 - acc: 0.9696 - val_loss: 0.4435 - val_acc: 0.8517\n",
            "Epoch 4/15\n",
            "1873/1873 [==============================] - 1s 382us/step - loss: 0.1093 - acc: 0.9845 - val_loss: 0.5173 - val_acc: 0.8278\n",
            "Epoch 5/15\n",
            "1873/1873 [==============================] - 1s 377us/step - loss: 0.0681 - acc: 0.9941 - val_loss: 0.5487 - val_acc: 0.8230\n",
            "Epoch 6/15\n",
            "1873/1873 [==============================] - 1s 389us/step - loss: 0.0513 - acc: 0.9979 - val_loss: 0.5798 - val_acc: 0.8134\n",
            "Epoch 7/15\n",
            "1873/1873 [==============================] - 1s 384us/step - loss: 0.0428 - acc: 0.9979 - val_loss: 0.6125 - val_acc: 0.8134\n",
            "Epoch 8/15\n",
            "1873/1873 [==============================] - 1s 383us/step - loss: 0.0377 - acc: 0.9984 - val_loss: 0.6451 - val_acc: 0.8134\n",
            "Epoch 9/15\n",
            "1873/1873 [==============================] - 1s 378us/step - loss: 0.0346 - acc: 0.9984 - val_loss: 0.6704 - val_acc: 0.8134\n",
            "Epoch 10/15\n",
            "1873/1873 [==============================] - 1s 381us/step - loss: 0.0325 - acc: 0.9984 - val_loss: 0.6965 - val_acc: 0.8086\n",
            "Epoch 11/15\n",
            "1873/1873 [==============================] - 1s 378us/step - loss: 0.0311 - acc: 0.9984 - val_loss: 0.7229 - val_acc: 0.8134\n",
            "Epoch 12/15\n",
            "1873/1873 [==============================] - 1s 383us/step - loss: 0.0301 - acc: 0.9984 - val_loss: 0.7442 - val_acc: 0.8134\n",
            "Epoch 13/15\n",
            "1873/1873 [==============================] - 1s 372us/step - loss: 0.0293 - acc: 0.9984 - val_loss: 0.7657 - val_acc: 0.8182\n",
            "Epoch 14/15\n",
            "1873/1873 [==============================] - 1s 374us/step - loss: 0.0287 - acc: 0.9984 - val_loss: 0.7852 - val_acc: 0.8134\n",
            "Epoch 15/15\n",
            "1873/1873 [==============================] - 1s 384us/step - loss: 0.0283 - acc: 0.9984 - val_loss: 0.8003 - val_acc: 0.8182\n",
            "232/232 [==============================] - 0s 129us/step\n",
            "\n",
            " all labeled: (random would be 0.058)\n",
            "Train on 3865 samples, validate on 430 samples\n",
            "Epoch 1/15\n",
            "3865/3865 [==============================] - 1s 374us/step - loss: nan - acc: 0.3190 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 2/15\n",
            "3865/3865 [==============================] - 1s 380us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 3/15\n",
            "3865/3865 [==============================] - 1s 373us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 4/15\n",
            "3865/3865 [==============================] - 1s 376us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 5/15\n",
            "3865/3865 [==============================] - 1s 370us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 6/15\n",
            "3865/3865 [==============================] - 1s 375us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 7/15\n",
            "3865/3865 [==============================] - 1s 378us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 8/15\n",
            "3865/3865 [==============================] - 1s 377us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 9/15\n",
            "3865/3865 [==============================] - 1s 388us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 10/15\n",
            "3865/3865 [==============================] - 1s 387us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 11/15\n",
            "3865/3865 [==============================] - 2s 395us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 12/15\n",
            "3865/3865 [==============================] - 1s 382us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 13/15\n",
            "3865/3865 [==============================] - 1s 375us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 14/15\n",
            "3865/3865 [==============================] - 1s 381us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 15/15\n",
            "3865/3865 [==============================] - 1s 376us/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "478/478 [==============================] - 0s 119us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan, 0.29288702941339884]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "fNKcipQnFXJg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "IJ9Fu0ZD3D--",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0000001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S48iNfma3G7a",
        "colab_type": "code",
        "outputId": "17eb2e7c-36b2-4506-9ba4-b9cdd60695c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3026
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print('\\n two labeled: (random would be .5)')\n",
        "model.fit(mfcc_train_two, y_train_two, epochs=20, validation_split=0.1, batch_size=4)\n",
        "model.evaluate(mfcc_test_two, y_test_two)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0000001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('\\n three labeled: (random would be .33)')\n",
        "model.fit(mfcc_train_three, y_train_three, epochs=20, validation_split=0.1, batch_size=4)\n",
        "model.evaluate(mfcc_test_three, y_test_three)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0000001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "print('\\n four labeled: (random would be .25)')\n",
        "model.fit(mfcc_train_four, y_train_four, epochs=20, validation_split=0.1, batch_size=4)\n",
        "model.evaluate(mfcc_test_four, y_test_four)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0000001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('\\n all labeled: (random would be .058)')\n",
        "model.fit(mfcc_train_all, y_train_all, epochs=20, validation_split=0.1, batch_size=4)\n",
        "model.evaluate(mfcc_test_all, y_test_all)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " two labeled: (random would be .5)\n",
            "Train on 3865 samples, validate on 430 samples\n",
            "Epoch 1/20\n",
            "3865/3865 [==============================] - 10s 2ms/step - loss: 9.0134 - acc: 0.3700 - val_loss: 6.9630 - val_acc: 0.5163\n",
            "Epoch 2/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: 6.3106 - acc: 0.5731 - val_loss: 5.8343 - val_acc: 0.6186\n",
            "Epoch 3/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: 5.6191 - acc: 0.6360 - val_loss: 5.4409 - val_acc: 0.6442\n",
            "Epoch 4/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: 5.3597 - acc: 0.6577 - val_loss: 5.2613 - val_acc: 0.6651\n",
            "Epoch 5/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: 5.2447 - acc: 0.6688 - val_loss: 5.1735 - val_acc: 0.6767\n",
            "Epoch 6/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: 5.1623 - acc: 0.6750 - val_loss: 5.1432 - val_acc: 0.6791\n",
            "Epoch 7/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: 5.1168 - acc: 0.6799 - val_loss: 5.1345 - val_acc: 0.6814\n",
            "Epoch 8/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: 5.0938 - acc: 0.6815 - val_loss: 5.1288 - val_acc: 0.6814\n",
            "Epoch 9/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: 5.0835 - acc: 0.6831 - val_loss: 5.1254 - val_acc: 0.6814\n",
            "Epoch 10/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: 5.0792 - acc: 0.6836 - val_loss: 5.1225 - val_acc: 0.6814\n",
            "Epoch 11/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: 5.0770 - acc: 0.6846 - val_loss: 5.1201 - val_acc: 0.6814\n",
            "Epoch 12/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: 5.0758 - acc: 0.6849 - val_loss: 5.1183 - val_acc: 0.6814\n",
            "Epoch 13/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: 5.0753 - acc: 0.6851 - val_loss: 5.1169 - val_acc: 0.6814\n",
            "Epoch 14/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: 5.0745 - acc: 0.6851 - val_loss: 5.1158 - val_acc: 0.6814\n",
            "Epoch 15/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: 5.0737 - acc: 0.6851 - val_loss: 5.1148 - val_acc: 0.6814\n",
            "Epoch 16/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: 5.0730 - acc: 0.6851 - val_loss: 5.1137 - val_acc: 0.6814\n",
            "Epoch 17/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: 5.0723 - acc: 0.6851 - val_loss: 5.1127 - val_acc: 0.6814\n",
            "Epoch 18/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: 5.0716 - acc: 0.6851 - val_loss: 5.1118 - val_acc: 0.6814\n",
            "Epoch 19/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: 5.0712 - acc: 0.6854 - val_loss: 5.1112 - val_acc: 0.6814\n",
            "Epoch 20/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: 5.0711 - acc: 0.6854 - val_loss: 5.1109 - val_acc: 0.6814\n",
            "478/478 [==============================] - 0s 94us/step\n",
            "\n",
            " three labeled: (random would be .33)\n",
            "Train on 2857 samples, validate on 318 samples\n",
            "Epoch 1/20\n",
            "2857/2857 [==============================] - 7s 3ms/step - loss: 12.7355 - acc: 0.1439 - val_loss: 10.1304 - val_acc: 0.3208\n",
            "Epoch 2/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.7394 - acc: 0.3598 - val_loss: 9.1176 - val_acc: 0.4245\n",
            "Epoch 3/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.4220 - acc: 0.4067 - val_loss: 9.0365 - val_acc: 0.4340\n",
            "Epoch 4/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.3624 - acc: 0.4137 - val_loss: 9.0019 - val_acc: 0.4371\n",
            "Epoch 5/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.3286 - acc: 0.4176 - val_loss: 8.9783 - val_acc: 0.4403\n",
            "Epoch 6/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.3056 - acc: 0.4190 - val_loss: 8.9658 - val_acc: 0.4403\n",
            "Epoch 7/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2886 - acc: 0.4207 - val_loss: 8.9605 - val_acc: 0.4434\n",
            "Epoch 8/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2753 - acc: 0.4225 - val_loss: 8.9598 - val_acc: 0.4434\n",
            "Epoch 9/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2674 - acc: 0.4239 - val_loss: 8.9598 - val_acc: 0.4434\n",
            "Epoch 10/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2618 - acc: 0.4239 - val_loss: 8.9599 - val_acc: 0.4434\n",
            "Epoch 11/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2570 - acc: 0.4242 - val_loss: 8.9599 - val_acc: 0.4434\n",
            "Epoch 12/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2533 - acc: 0.4246 - val_loss: 8.9599 - val_acc: 0.4434\n",
            "Epoch 13/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2503 - acc: 0.4249 - val_loss: 8.9599 - val_acc: 0.4434\n",
            "Epoch 14/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2484 - acc: 0.4256 - val_loss: 8.9598 - val_acc: 0.4434\n",
            "Epoch 15/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2474 - acc: 0.4256 - val_loss: 8.9597 - val_acc: 0.4434\n",
            "Epoch 16/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2469 - acc: 0.4263 - val_loss: 8.9597 - val_acc: 0.4434\n",
            "Epoch 17/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2468 - acc: 0.4263 - val_loss: 8.9596 - val_acc: 0.4434\n",
            "Epoch 18/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2467 - acc: 0.4263 - val_loss: 8.9595 - val_acc: 0.4434\n",
            "Epoch 19/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2467 - acc: 0.4263 - val_loss: 8.9595 - val_acc: 0.4434\n",
            "Epoch 20/20\n",
            "2857/2857 [==============================] - 6s 2ms/step - loss: 9.2466 - acc: 0.4263 - val_loss: 8.9594 - val_acc: 0.4434\n",
            "353/353 [==============================] - 0s 109us/step\n",
            "\n",
            " four labeled: (random would be .25)\n",
            "Train on 1873 samples, validate on 209 samples\n",
            "Epoch 1/20\n",
            "1873/1873 [==============================] - 5s 3ms/step - loss: 7.0431 - acc: 0.5040 - val_loss: 6.3045 - val_acc: 0.5646\n",
            "Epoch 2/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 6.0671 - acc: 0.5985 - val_loss: 5.9224 - val_acc: 0.6220\n",
            "Epoch 3/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.8004 - acc: 0.6247 - val_loss: 5.7974 - val_acc: 0.6316\n",
            "Epoch 4/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6954 - acc: 0.6348 - val_loss: 5.7210 - val_acc: 0.6364\n",
            "Epoch 5/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6617 - acc: 0.6444 - val_loss: 5.6742 - val_acc: 0.6364\n",
            "Epoch 6/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6451 - acc: 0.6460 - val_loss: 5.6362 - val_acc: 0.6364\n",
            "Epoch 7/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6336 - acc: 0.6471 - val_loss: 5.6026 - val_acc: 0.6364\n",
            "Epoch 8/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6247 - acc: 0.6482 - val_loss: 5.5758 - val_acc: 0.6411\n",
            "Epoch 9/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6173 - acc: 0.6487 - val_loss: 5.5598 - val_acc: 0.6459\n",
            "Epoch 10/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6111 - acc: 0.6498 - val_loss: 5.5523 - val_acc: 0.6507\n",
            "Epoch 11/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6067 - acc: 0.6503 - val_loss: 5.5483 - val_acc: 0.6507\n",
            "Epoch 12/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6044 - acc: 0.6514 - val_loss: 5.5461 - val_acc: 0.6507\n",
            "Epoch 13/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6033 - acc: 0.6519 - val_loss: 5.5448 - val_acc: 0.6507\n",
            "Epoch 14/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6028 - acc: 0.6524 - val_loss: 5.5439 - val_acc: 0.6507\n",
            "Epoch 15/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6026 - acc: 0.6524 - val_loss: 5.5429 - val_acc: 0.6507\n",
            "Epoch 16/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6025 - acc: 0.6524 - val_loss: 5.5421 - val_acc: 0.6507\n",
            "Epoch 17/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6024 - acc: 0.6524 - val_loss: 5.5414 - val_acc: 0.6507\n",
            "Epoch 18/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6023 - acc: 0.6524 - val_loss: 5.5407 - val_acc: 0.6507\n",
            "Epoch 19/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6023 - acc: 0.6524 - val_loss: 5.5400 - val_acc: 0.6507\n",
            "Epoch 20/20\n",
            "1873/1873 [==============================] - 4s 2ms/step - loss: 5.6023 - acc: 0.6524 - val_loss: 5.5392 - val_acc: 0.6507\n",
            "232/232 [==============================] - 0s 114us/step\n",
            "\n",
            " all labeled: (random would be .058)\n",
            "Train on 3865 samples, validate on 430 samples\n",
            "Epoch 1/20\n",
            "3865/3865 [==============================] - 10s 3ms/step - loss: nan - acc: 0.3208 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 2/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 3/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 4/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 5/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 6/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 7/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 8/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 9/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 10/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 11/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 12/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 13/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 14/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 15/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 16/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 17/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 18/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 19/20\n",
            "3865/3865 [==============================] - 8s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 20/20\n",
            "3865/3865 [==============================] - 9s 2ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "478/478 [==============================] - 0s 95us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan, 0.29288702941339884]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "1lj46UGa4PZ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D, BatchNormalization\n",
        "\n",
        "input_shape = (16000, 1)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape, dilation_rate = 1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', dilation_rate = 2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', dilation_rate = 4))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling1D(pool_size=8))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qgBt8M2w4SPi",
        "colab_type": "code",
        "outputId": "323ff738-387d-41fa-d392-63f5871c5dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1666
        }
      },
      "cell_type": "code",
      "source": [
        "print('\\n two labeled: (random would be .5)')\n",
        "audio_train_cnn_two = audio_train_two[:, :, np.newaxis]\n",
        "audio_test_cnn_two = audio_test_two[:, :, np.newaxis]\n",
        "model.fit(audio_train_cnn_two, y_train_two, epochs=10, validation_split=0.1, batch_size=32)\n",
        "model.evaluate(audio_test_cnn_two, y_test_two)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape, dilation_rate = 1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', dilation_rate = 2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', dilation_rate = 4))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling1D(pool_size=8))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('\\n three labeled: (random would be .33)')\n",
        "audio_train_cnn_three = audio_train_three[:, :, np.newaxis]\n",
        "audio_test_cnn_three = audio_test_three[:, :, np.newaxis]\n",
        "model.fit(audio_train_cnn_three, y_train_three, epochs=10, validation_split=0.1, batch_size=32)\n",
        "model.evaluate(audio_test_cnn_three, y_test_three)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape, dilation_rate = 1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', dilation_rate = 2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', dilation_rate = 4))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling1D(pool_size=8))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "print('\\n four labeled: (random would be .25)')\n",
        "audio_train_cnn_four = audio_train_four[:, :, np.newaxis]\n",
        "audio_test_cnn_four = audio_test_four[:, :, np.newaxis]\n",
        "model.fit(audio_train_cnn_four, y_train_four, epochs=10, validation_split=0.1, batch_size=32)\n",
        "model.evaluate(audio_test_cnn_four, y_test_four)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape, dilation_rate = 1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', dilation_rate = 2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', dilation_rate = 4))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling1D(pool_size=8))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "print('\\n all labeled: (random would be .058)')\n",
        "audio_train_cnn_all = audio_train_all[:, :, np.newaxis]\n",
        "audio_test_cnn_all = audio_test_all[:, :, np.newaxis]\n",
        "model.fit(audio_train_cnn_all, y_train_all, epochs=10, validation_split=0.1, batch_size=32)\n",
        "model.evaluate(audio_test_cnn_all, y_test_all)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " two labeled: (random would be .5)\n",
            "Train on 3865 samples, validate on 430 samples\n",
            "Epoch 1/10\n",
            "3865/3865 [==============================] - 21s 5ms/step - loss: 1.0299 - acc: 0.6171 - val_loss: 0.6302 - val_acc: 0.6837\n",
            "Epoch 2/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: 0.6171 - acc: 0.6911 - val_loss: 0.6553 - val_acc: 0.6721\n",
            "Epoch 3/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: 0.5771 - acc: 0.7190 - val_loss: 0.7931 - val_acc: 0.3535\n",
            "Epoch 4/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: 0.5530 - acc: 0.7309 - val_loss: 0.6584 - val_acc: 0.6977\n",
            "Epoch 5/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: 0.4997 - acc: 0.7622 - val_loss: 0.8444 - val_acc: 0.5163\n",
            "Epoch 6/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: 0.4364 - acc: 0.8070 - val_loss: 1.0510 - val_acc: 0.6860\n",
            "Epoch 7/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: 0.4348 - acc: 0.8036 - val_loss: 5.9793 - val_acc: 0.3395\n",
            "Epoch 8/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: 0.3706 - acc: 0.8455 - val_loss: 1.1284 - val_acc: 0.5651\n",
            "Epoch 9/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: 0.4800 - acc: 0.7715 - val_loss: 1.7042 - val_acc: 0.3791\n",
            "Epoch 10/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: 0.3354 - acc: 0.8541 - val_loss: 7.8860 - val_acc: 0.3279\n",
            "478/478 [==============================] - 1s 1ms/step\n",
            "\n",
            " three labeled: (random would be .33)\n",
            "Train on 2857 samples, validate on 318 samples\n",
            "Epoch 1/10\n",
            "2857/2857 [==============================] - 15s 5ms/step - loss: 1.5248 - acc: 0.3882 - val_loss: 1.0350 - val_acc: 0.4151\n",
            "Epoch 2/10\n",
            "2857/2857 [==============================] - 12s 4ms/step - loss: 1.0129 - acc: 0.4956 - val_loss: 1.3444 - val_acc: 0.4151\n",
            "Epoch 3/10\n",
            "2857/2857 [==============================] - 12s 4ms/step - loss: 0.9641 - acc: 0.5408 - val_loss: 2.3368 - val_acc: 0.4151\n",
            "Epoch 4/10\n",
            "2857/2857 [==============================] - 12s 4ms/step - loss: 0.9172 - acc: 0.5646 - val_loss: 1.1859 - val_acc: 0.3962\n",
            "Epoch 5/10\n",
            "2857/2857 [==============================] - 12s 4ms/step - loss: 0.8511 - acc: 0.6069 - val_loss: 1.0062 - val_acc: 0.4969\n",
            "Epoch 6/10\n",
            "2857/2857 [==============================] - 12s 4ms/step - loss: 0.7822 - acc: 0.6517 - val_loss: 1.8091 - val_acc: 0.4591\n",
            "Epoch 7/10\n",
            "2857/2857 [==============================] - 12s 4ms/step - loss: 0.7016 - acc: 0.6923 - val_loss: 1.9006 - val_acc: 0.4434\n",
            "Epoch 8/10\n",
            "2857/2857 [==============================] - 12s 4ms/step - loss: 0.5141 - acc: 0.7921 - val_loss: 2.7345 - val_acc: 0.4308\n",
            "Epoch 9/10\n",
            "2857/2857 [==============================] - 12s 4ms/step - loss: 0.4000 - acc: 0.8383 - val_loss: 8.8369 - val_acc: 0.4245\n",
            "Epoch 10/10\n",
            "2857/2857 [==============================] - 12s 4ms/step - loss: 0.3381 - acc: 0.8733 - val_loss: 1.8621 - val_acc: 0.4497\n",
            "353/353 [==============================] - 1s 1ms/step\n",
            "\n",
            " four labeled: (random would be .25)\n",
            "Train on 1873 samples, validate on 209 samples\n",
            "Epoch 1/10\n",
            "1873/1873 [==============================] - 11s 6ms/step - loss: 1.7099 - acc: 0.5339 - val_loss: 1.2744 - val_acc: 0.6555\n",
            "Epoch 2/10\n",
            "1873/1873 [==============================] - 8s 4ms/step - loss: 1.0344 - acc: 0.6412 - val_loss: 2.5886 - val_acc: 0.6555\n",
            "Epoch 3/10\n",
            "1873/1873 [==============================] - 8s 4ms/step - loss: 0.9129 - acc: 0.6647 - val_loss: 3.8390 - val_acc: 0.6555\n",
            "Epoch 4/10\n",
            "1873/1873 [==============================] - 8s 4ms/step - loss: 0.8372 - acc: 0.6845 - val_loss: 1.6751 - val_acc: 0.6555\n",
            "Epoch 5/10\n",
            "1873/1873 [==============================] - 8s 4ms/step - loss: 0.7758 - acc: 0.6946 - val_loss: 3.7046 - val_acc: 0.6555\n",
            "Epoch 6/10\n",
            "1873/1873 [==============================] - 8s 4ms/step - loss: 0.7131 - acc: 0.7293 - val_loss: 1.6524 - val_acc: 0.6555\n",
            "Epoch 7/10\n",
            "1873/1873 [==============================] - 8s 4ms/step - loss: 0.6839 - acc: 0.7330 - val_loss: 3.6475 - val_acc: 0.6555\n",
            "Epoch 8/10\n",
            "1873/1873 [==============================] - 8s 4ms/step - loss: 0.5735 - acc: 0.7832 - val_loss: 2.8423 - val_acc: 0.6555\n",
            "Epoch 9/10\n",
            "1873/1873 [==============================] - 8s 4ms/step - loss: 0.4882 - acc: 0.8105 - val_loss: 5.4198 - val_acc: 0.6555\n",
            "Epoch 10/10\n",
            "1873/1873 [==============================] - 8s 4ms/step - loss: 0.4966 - acc: 0.8142 - val_loss: 3.3261 - val_acc: 0.1483\n",
            "232/232 [==============================] - 0s 1ms/step\n",
            "\n",
            " all labeled: (random would be .058)\n",
            "Train on 3865 samples, validate on 430 samples\n",
            "Epoch 1/10\n",
            "3865/3865 [==============================] - 19s 5ms/step - loss: nan - acc: 0.3198 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 2/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 3/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 4/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 5/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 6/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 7/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 8/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 9/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "Epoch 10/10\n",
            "3865/3865 [==============================] - 16s 4ms/step - loss: nan - acc: 0.3211 - val_loss: nan - val_acc: 0.3000\n",
            "478/478 [==============================] - 1s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan, 0.29288702941339884]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "wTtSabl8WCqE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "input_shape = (16000, 1)\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(Embedding(1, output_dim=256))\n",
        "model.add(LSTM(128, input_shape=input_shape))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vm1Muo3TWFi6",
        "colab_type": "code",
        "outputId": "1b0cf794-31cc-48d5-f9b1-a0dab8e66f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "print('\\n two labeled: (random would be .5)')\n",
        "audio_train_cnn_two = audio_train_two[:, :, np.newaxis]\n",
        "audio_test_cnn_two = audio_test_two[:, :, np.newaxis]\n",
        "model.fit(audio_train_cnn_two, y_train_two, epochs=10, validation_split=0.1, batch_size=8)\n",
        "model.evaluate(audio_test_cnn_two, y_test_two)\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(Embedding(1, output_dim=256))\n",
        "model.add(LSTM(128, input_shape=input_shape))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('\\n three labeled: (random would be .33)')\n",
        "audio_train_cnn_three = audio_train_three[:, :, np.newaxis]\n",
        "audio_test_cnn_three = audio_test_three[:, :, np.newaxis]\n",
        "model.fit(audio_train_cnn_three, y_train_three, epochs=10, validation_split=0.1, batch_size=8)\n",
        "model.evaluate(audio_test_cnn_three, y_test_three)\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(Embedding(1, output_dim=256))\n",
        "model.add(LSTM(128, input_shape=input_shape))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('\\n four labeled: (random would be .25)')\n",
        "audio_train_cnn_four = audio_train_four[:, :, np.newaxis]\n",
        "audio_test_cnn_four = audio_test_four[:, :, np.newaxis]\n",
        "model.fit(audio_train_cnn_four, y_train_four, epochs=10, validation_split=0.1, batch_size=8)\n",
        "model.evaluate(audio_test_cnn_four, y_test_four)\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(Embedding(1, output_dim=256))\n",
        "model.add(LSTM(128, input_shape=input_shape))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('\\n four labeled: (random would be .058)')\n",
        "audio_train_cnn_all = audio_train_all[:, :, np.newaxis]\n",
        "audio_test_cnn_all = audio_test_all[:, :, np.newaxis]\n",
        "model.fit(audio_train_cnn_all, y_train_all, epochs=10, validation_split=0.1, batch_size=8)\n",
        "model.evaluate(audio_test_cnn_all, y_test_all)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " two labeled: (random would be .5)\n",
            "Train on 3865 samples, validate on 430 samples\n",
            "Epoch 1/10\n",
            " 728/3865 [====>.........................] - ETA: 4:57:41 - loss: 0.7637 - acc: 0.6168"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}